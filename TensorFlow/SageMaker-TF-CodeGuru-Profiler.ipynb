{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and push image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=sagemaker-tf-profiler\n",
    "\n",
    "cd container\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-east-1 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-east-1}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email --registry-ids 763104351884 )\n",
    "#$(aws ecr get-login --region us-east-1 --no-include-email --registry-ids 763104351884)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build  -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = boto3.Session().region_name\n",
    "image_uri = \"{}.dkr.ecr.{}.amazonaws.com/sagemaker-tf-profiler:latest\".format(account_id,region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Feature Group and ingest \n",
    "Below we generate some random input data for our model and ingest it into Feature Store.\n",
    "We do this as our inference.py script will confirm that the input we are sending to our model for prediction is the latest features we saved in Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import gmtime, strftime\n",
    "import time\n",
    "\n",
    "\n",
    "random_input = np.random.rand(1, 1, 3, 3)\n",
    "random_input = random_input.tolist()\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df[\"RandomInput\"] = [random_input]\n",
    "df = df.astype({'RandomInput': 'string'})\n",
    "\n",
    "current_time_sec = int(round(time.time()))\n",
    "df[\"EventTime\"] = current_time_sec\n",
    "df = df.astype({'EventTime': 'float64'})\n",
    "df['Index'] = range(1, len(df) + 1)\n",
    "df[\"RandomInput\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.session import Session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "prefix = 'DEMO-CodeGuru-Profiler'\n",
    "my_features_feature_group = FeatureGroup(\n",
    "    name=\"my-features\", sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "\n",
    "my_features_feature_group.load_feature_definitions(data_frame=df)\n",
    "\n",
    "record_identifier_feature_name=\"RandomInput\"\n",
    "\n",
    "my_features_feature_group.create(\n",
    "    s3_uri=f\"s3://{bucket}/{prefix}\",\n",
    "    record_identifier_name=record_identifier_feature_name,\n",
    "    event_time_feature_name=\"EventTime\",\n",
    "    role_arn=role,\n",
    "    enable_online_store=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_feature_group_status(feature_group):\n",
    "    status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for Feature Group to be Created\")\n",
    "        time.sleep(5)\n",
    "        status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    print(f\"FeatureGroup {feature_group.name} successfully created.\")\n",
    "\n",
    "\n",
    "check_feature_group_status(my_features_feature_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_features_feature_group.ingest(data_frame=df, max_workers=3, wait=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an inference.py\n",
    "Here we create an inference.py with an expensive function that retrieves the latest feature value from Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference.py\n",
    "\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "\n",
    "def retrieve_latest_features_expensive(data,record_id):\n",
    "    import boto3\n",
    "\n",
    "    client = boto3.client('sagemaker-featurestore-runtime')\n",
    "\n",
    "    response = client.get_record(\n",
    "    FeatureGroupName='my-features',\n",
    "    RecordIdentifierValueAsString=str(record_id),\n",
    "\n",
    "    )\n",
    "    return response[\"Record\"][0][\"ValueAsString\"]\n",
    "    \n",
    "\n",
    "def input_handler(data, context):\n",
    "    \"\"\" Pre-process request input before it is sent to TensorFlow Serving REST API\n",
    "    Args:\n",
    "        data (obj): the request data, in format of dict or string\n",
    "        context (Context): an object containing request and configuration details\n",
    "    Returns:\n",
    "        (dict): a JSON-serializable dict that contains request body and headers\n",
    "    \"\"\"\n",
    "\n",
    "    if context.request_content_type == 'application/json':\n",
    "\n",
    "        d = data.read().decode('utf-8')\n",
    "      \n",
    "        input_data = json.loads(d)\n",
    "        print(\"Prediction features recieved from InvokeEndpoint API:\")\n",
    "        print(type(input_data[\"data\"]))\n",
    "        print(\"Features recieved from Feature Store:\")\n",
    "        print(type(retrieve_latest_features_expensive(input_data[\"data\"],json.loads(input_data[\"id\"]))))\n",
    "              \n",
    "        assert input_data[\"data\"] == json.loads(retrieve_latest_features_expensive(input_data[\"data\"],input_data[\"id\"]))\n",
    "      \n",
    "        return json.dumps({\"inputs\" : input_data[\"data\"]})\n",
    "\n",
    "\n",
    "    raise ValueError('{{\"error\": \"unsupported content type {}\"}}'.format(\n",
    "        context.request_content_type or \"unknown\"))\n",
    "\n",
    "\n",
    "def output_handler(data, context):\n",
    "    \"\"\"Post-process TensorFlow Serving output before it is returned to the client.\n",
    "    Args:\n",
    "        data (obj): the TensorFlow serving response\n",
    "        context (Context): an object containing request and configuration details\n",
    "    Returns:\n",
    "        (bytes, string): data to return to client, response content type\n",
    "    \"\"\"\n",
    "    if data.status_code != 200:\n",
    "        raise ValueError(data.content.decode('utf-8'))\n",
    "    response_content_type = context.accept_header\n",
    "    prediction = data.content\n",
    "    return prediction, response_content_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy our Model to an Endpoint\n",
    "Our container has been pushed to ECR and our Model is in S3 now we have everything we need to Deploy to a SageMaker Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "saved_model = (\n",
    "    \"s3://sagemaker-sample-data-{}/tensorflow/model/resnet/resnet_50_v2_fp32_NCHW.tar.gz\".format(\n",
    "        region\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.serving import Model\n",
    "from sagemaker.local import LocalSession\n",
    "\n",
    "tensorflow_model = Model(model_data=saved_model,\n",
    "                         role=role,\n",
    "                         entry_point = \"inference.py\",\n",
    "                         image_uri=image_uri,\n",
    "                         env={\"AWS_DEFAULT_REGION\": \"us-east-1\"},\n",
    "                        sagemaker_session = LocalSession()\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictor = tensorflow_model.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.t2.medium\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "input_data = {\n",
    "    \"data\": random_input,\n",
    "    \"id\" : \"1\"\n",
    "}\n",
    "#for i in range (0,20000):\n",
    "while True:\n",
    "    #predictor.serializer = ser\n",
    "    prediction = predictor.predict(input_data)\n",
    "    print(prediction)\n",
    "    break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
